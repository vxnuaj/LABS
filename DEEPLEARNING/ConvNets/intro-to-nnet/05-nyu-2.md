#  Stochastic Gradient Descent & Backpropagation

> *Learning resource:  [Lecture: Stochastic gradient descent and backpropagation](https://www.youtube.com/watch?v=d9vdh3b787Y&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=4)*

### Gradient Descent

Gradient descent is about finding the minima of your loss function.

Stochastic gradient descent involves taking a single sample, and **then** computing the loss function with respect to a parameter. Not the full batch nor a mini-batch.

They assume and will work best if your loss function is continuous, meaning differentiable over it's entire surface and monotonically increasing / decreasing.

Think of gradient descent as a man trying to find the bottom of a valley, on his way down from a foresty mountain.

Formulate it as:

$\theta = \theta - \eta\frac{∂\mathcal{L}}{∂\theta}$

where $\frac{∂\mathcal{L}}{∂\theta}$ is the graident of the loss with respect to the given parameter, $\theta$.

The gradient is positive for an unoptimized model, and thereby we subtract this gradient from the given $\theta$ over multiple iterations until the gradient is equal to $0$ (as $\theta - 0 = \theta$).

At the point where the gradient is equal to $0$, we have found the optima in the solution space.

The gradient of the loss function is a good representation for optimizing the model as the common loss function, cross-entropy, as a minima at $0$, where it cannot decrease further.

Then at $0$, the model has found the best solution space with a gradient of $0$.

Each $∂$, for earlier layers if needbe, is found via derivatives and the chain rule.

Say you have $\frac{∂\mathcal{L}}{∂\theta_2}$ and want to get $\frac{∂\mathcal{L}}{∂\theta_1}$, you'd compute it as:

$\frac{∂\mathcal{L}}{∂\theta_1} = (\frac{∂\mathcal{L}}{∂\theta_2})(\frac{∂\theta_2}{∂\theta_1})$

and so an and so forth for the given $∂\theta_i$ you want.