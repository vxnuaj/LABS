{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Linear import mse, init_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of J: (1, 127)\n",
      "First val of J: 2.92028892055268\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('data/random1.csv')\n",
    "data = np.array(data)\n",
    "\n",
    "X_train = data[:, :1].T\n",
    "Y_train = data[:, 1].reshape(1, -1)\n",
    "\n",
    "w, b = init_params()\n",
    "\n",
    "def forward(x, w):\n",
    "    pred = np.dot(w, x)\n",
    "    return pred\n",
    "\n",
    "J = forward(X_train, w )\n",
    "print(f\"Shape of J: {J.shape}\") \n",
    "print(f\"First val of J: {J[0, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards(x, y, pred):\n",
    "    dw = x\n",
    "    return dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First val of DW: 1.5\n"
     ]
    }
   ],
   "source": [
    "grad = backwards(X_train, Y_train, J)\n",
    "\n",
    "print(f\"First val of DW: {grad[0,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<Screenshot 2024-06-07 at 9.15.13â€¯AM.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4\n",
      "11.400000001771105\n",
      "\n",
      "Avg Difference : 0.00000000039643172910\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-7\n",
    "\n",
    "wplus = w + eps\n",
    "wminus = w - eps\n",
    "\n",
    "jplus = forward(X_train, wplus)\n",
    "jminus = forward(X_train, wminus)\n",
    "\n",
    "gradapprox = (jplus - jminus) / (2 * eps)\n",
    "\n",
    "\n",
    "difference_num = np.linalg.norm((grad - gradapprox), ord=2)\n",
    "graddifference_denum = np.linalg.norm(grad, ord = 2)\n",
    "gradapproxdifference_denum = np.linalg.norm(gradapprox, ord = 2)\n",
    "\n",
    "difference = (difference_num) / (graddifference_denum + gradapproxdifference_denum)\n",
    "\n",
    "print(grad[0, 10])\n",
    "print(gradapprox[0,10])\n",
    "\n",
    "print('\\nAvg Difference : {:.20f}'.format(difference))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import init_params, leaky_relu, leaky_relu_deriv, softmax, one_hot, cce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/fashion-mnist_train.csv')\n",
    "data = np.array(data)\n",
    "\n",
    "X_train = data[:, 1:785].T / 255\n",
    "Y_train = data[:, 0].reshape(1, -1)\n",
    "\n",
    "w1, b1, w2, b2 = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.302157472675863\n",
      "(64, 784)\n",
      "(10, 64)\n"
     ]
    }
   ],
   "source": [
    "def forward_n(Y, X, w1, b1, w2, b2):\n",
    "    \n",
    "    one_hot_y = one_hot(Y)\n",
    "\n",
    "    z1 = np.dot(w1, X) + b1\n",
    "    a1 = leaky_relu(z1)\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "\n",
    "    loss = cce(one_hot_y, a2)\n",
    "\n",
    "    return z1, a1, z2, a2, w1, b1, w2, b2, one_hot_y, loss\n",
    "\n",
    "z1, a1, z2, a2, w1, b1, w2, b2, one_hot_y, loss = forward_n(Y_train, X_train, w1, b1,w2, b2)\n",
    "print(loss)\n",
    "\n",
    "def backward_n(x, one_hot_y, w2, a2, a1, z1):\n",
    "    dz2 = a2 - one_hot_y\n",
    "    dw2 = np.dot(dz2, a1.T) / one_hot_y.shape[1]\n",
    "    db2 = np.sum(dz2, axis = 1, keepdims=True) / one_hot_y.shape[1]\n",
    "    dz1 = np.dot(w2.T, dz2) * leaky_relu_deriv(z1)\n",
    "    dw1 = np.dot(dz1, x.T) / one_hot_y.shape[1]\n",
    "    db1 = np.sum(dz1, axis = 1, keepdims=True) / one_hot_y.shape[1]\n",
    "    return dz1, db1, dw1, dz2, db2, dw2\n",
    "\n",
    "dz1, db1, dw1, dz2, db2, dw2 = backward_n(X_train, one_hot_y, w2, a2, a1, z1)\n",
    "print(dw1.shape)\n",
    "print(dw2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking DW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0037808267627781333\n",
      "0.0037808228651304778\n",
      "0.00000103089824002336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w1plus = w1.copy()\n",
    "w1minus = w1.copy()\n",
    "w2plus = w2.copy()\n",
    "w2minus = w2.copy()\n",
    "\n",
    "eps = 1e-7\n",
    "\n",
    "w2plus[0, 7] += eps\n",
    "w2minus[0, 7] -= eps\n",
    "\n",
    "z1, a1, z2, a2, w1, b1, w2plus, b2, one_hot_y, lossplus = forward_n(Y_train, X_train, w1, b1, w2plus, b2)\n",
    "z1, a1, z2, a2, w1, b1, w2minus, b2, one_hot_y, lossminus = forward_n(Y_train, X_train, w1, b1, w2minus, b2)\n",
    "\n",
    "gradapprox = (lossplus - lossminus) / (2 * eps)\n",
    "\n",
    "print(gradapprox)\n",
    "print(dw2[0,7])\n",
    "\n",
    "num = np.linalg.norm(dw2[0, 7] - gradapprox)\n",
    "denom = np.maximum(np.linalg.norm(dw2[0, 7]), np.linalg.norm(gradapprox))\n",
    "diff = num / denom\n",
    "\n",
    "print('{:.20f}'.format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking DW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00002517319686035080\n",
      "0.00002517492079837439\n",
      "0.00006847838916336178\n"
     ]
    }
   ],
   "source": [
    "w1plus[0, 7] += eps\n",
    "w1minus[0, 7] -= eps\n",
    "\n",
    "z1, a1, z2, a2, w1plus, b1, w2, b2, one_hot_y, lossplus = forward_n(Y_train, X_train, w1plus, b1, w2, b2)\n",
    "z1, a1, z2, a2, w1minus, b1, w2, b2, one_hot_y, lossminus = forward_n(Y_train, X_train, w1minus, b1, w2, b2)\n",
    "\n",
    "gradapprox = (lossplus - lossminus) / (2 * eps)\n",
    "\n",
    "num = np.linalg.norm(dw1[0, 7] - gradapprox)\n",
    "denom = np.maximum(np.linalg.norm(dw1[0, 7]), np.linalg.norm(gradapprox))\n",
    "diff = num / denom\n",
    "\n",
    "print('{:.20f}'.format(gradapprox))\n",
    "print('{:.20f}'.format(dw1[0,7]))\n",
    "print('{:.20f}'.format(diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
