{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instance 1\n",
    "\n",
    "The below shows the vanishing gradient problem.\n",
    "\n",
    "Where as inputs to sigmoid become very low or high, the output tends to be either at 0 or 1, which may result in a very small gradient nearing 0.\n",
    "\n",
    "When taking the gradient of the loss, w.r.t the prediction, using labels 0 ('grad w label0'), the  gradient is extremely small nearing 0.\n",
    "\n",
    "If used in deeper layers, this flat gradient can propagate throughout an entire model causing the learning to be ineffective and almost non existent.\n",
    "\n",
    "The gradient using labels 1, are then relatively high at -1. This implies that the output of sigmoid is very close to 0, reinforcing an issue with vanishing gradients.\n",
    "\n",
    "The root cause of this issue is be due to the magnitude of the input values which are on the order of thousands. Despite Xavier weight Initialization being used to mitigate a potential issue, the magnitude of the raw inputs were too high without much normalization.\n",
    "\n",
    "Therefore, a means to avoid the vanishing gradient problem is to normalize inputs to improve the stability of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw input: \n",
      "[[1000 1000]\n",
      " [2000 2000]\n",
      " [3000 3000]]\n",
      "weights: [[ 0.5390438890427759  -0.40388501506080915 -0.08320392462912277]]\n",
      "sigmiod input: [[-518.3379149662107 -518.3379149662107]]\n",
      "sigmoid output : [[7.739337196003053e-226 7.739337196003053e-226]]\n",
      "grad w label0: [[7.739337196003053e-226 7.739337196003053e-226]]\n",
      "grad w label1: [[-1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision = 50000)\n",
    "\n",
    "input = [[1000, 2000, 3000],[1000, 2000, 3000]]\n",
    "input = np.array(input) # 2 samples, 3 features\n",
    "input = input.T\n",
    "\n",
    "#w = np.random.randn(1, 3) * np.sqrt(1/3)\n",
    "\n",
    "w =  np.array([[ 0.5390438890427759, -0.40388501506080915, -0.08320392462912277]])\n",
    "\n",
    "b = 0\n",
    "\n",
    "z = np.dot(w, input) + b\n",
    "\n",
    "eps = 1e-10\n",
    "a = 1 / (1+np.exp(-z + eps))\n",
    "\n",
    "#using this loss:\n",
    "# bce = - np.sum(y * np.log(a) + (1- y) * np.log(1 - a)) / 3\n",
    "\n",
    "grad0 = a - np.array([0, 0])\n",
    "grad1 = a - np.array([1, 1])\n",
    "\n",
    "print(f\"raw input: \\n{input}\")\n",
    "print(f\"weights: {w}\")\n",
    "print(f\"sigmiod input: {z}\")\n",
    "print(f'sigmoid output : {a}')\n",
    "print(f'grad w label0: {grad0}')\n",
    "print(f'grad w label1: {grad1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
