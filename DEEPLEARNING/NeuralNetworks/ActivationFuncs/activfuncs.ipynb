{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid activation [[0.63638696 0.14832643]\n",
      " [0.33415799 0.679931  ]] \n",
      "\n",
      "sigmoid gradient [[0.2313986  0.1263257 ]\n",
      " [0.22249643 0.21762484]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(2,2)\n",
    "\n",
    "#SIGMOID\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "print(\"sigmoid activation\", sigmoid, \"\\n\")\n",
    "\n",
    "#SIGMOID GRAD\n",
    "sigmoid_grad = np.exp(-z) / ((1 + np.exp(-z))**2)\n",
    "print(\"sigmoid gradient\", sigmoid_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh activation [[-0.03171517 -0.17600671]\n",
      " [ 0.9724259   0.34117656]] \n",
      "\n",
      "tanh gradient [[0.99899415 0.96902164]\n",
      " [0.05438788 0.88359855]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(2,2)\n",
    "\n",
    "#TANH\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "a = tanh(z)\n",
    "\n",
    "print(\"tanh activation\", a, \"\\n\")\n",
    "\n",
    "#TANH GRAD\n",
    "\n",
    "tanh_grad = 1 - (tanh(z) ** 2)\n",
    "\n",
    "print(\"tanh gradient\" , tanh_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu activation: [[0.35432201 0.89517908]\n",
      " [0.         0.        ]]\n",
      "\n",
      "relu gradient [[1. 1.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(2,2)\n",
    "\n",
    "#RELU\n",
    "relu = np.maximum(0, z)\n",
    "\n",
    "print(f\"relu activation: {relu}\\n\")\n",
    "\n",
    "#RELU GRAD\n",
    "relu_grad = np.float16(z > 0)\n",
    "print(f\"relu gradient {relu_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaky relu [[ 1.76377819 -0.00970334]\n",
      " [-0.02032992 -0.02334202]]\n",
      "\n",
      "leaky_relu_grad [[1.  0.1]\n",
      " [0.1 0.1]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(2,2)\n",
    "\n",
    "#Leaky ReLU\n",
    "leaky_relu = np.where(z > 0, z, (.01 * z))\n",
    "print(f\"leaky relu {leaky_relu}\\n\")\n",
    "\n",
    "#Leaky ReLU Grad\n",
    "leaky_relu_grad = np.where(z>0, 1, .1)\n",
    "print(f\"leaky_relu_grad {leaky_relu_grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57344878 0.30405545]\n",
      " [0.42655122 0.69594455]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(2,2)\n",
    "\n",
    "#SOFTMAX\n",
    "softmax = np.exp(z) / np.sum(np.exp(z), axis=0, keepdims=True)\n",
    "#Assuming that axis 0 is the number of classes. Otherwise, axis = 1.\n",
    "\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elu activation: [[ 0.45094934 -0.00692517]\n",
      " [ 0.17574772 -0.00522037]]\n",
      "elu2 activation: [[ 0.45094934 -0.00692517]\n",
      " [ 0.17574772 -0.00522037]]\n",
      "\n",
      "elu gradient:[[1.         0.00307483]\n",
      " [1.         0.00477963]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(2,2)\n",
    "\n",
    "alpha = .01\n",
    "\n",
    "#ELU | Either works\n",
    "elu = np.maximum(z, (alpha * (np.exp(z) - 1)))\n",
    "elu2 = np.where(z > 0, z, (alpha * (np.exp(z) - 1)))\n",
    "print(f\"elu activation: {elu}\")\n",
    "print(f\"elu2 activation: {elu2}\")\n",
    "\n",
    "#ELU_gradient\n",
    "elu_grad1 = np.where(z > 0, 1, (alpha * np.exp(z)))\n",
    "print(f\"\\nelu gradient:{elu_grad1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selu activation[[ 0.74985001 -0.68102427]\n",
      " [ 0.60075593  0.55670632]]\n",
      "\n",
      "selu_deriv [[1.0505     1.07667233]\n",
      " [1.0505     1.0505    ]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(2,2)\n",
    "\n",
    "alpha = 1.6732\n",
    "lambd = 1.0505\n",
    "selu = np.where(z > 0, lambd * (z), lambd *(alpha *(np.exp(z) - 1)))\n",
    "\n",
    "print(f\"selu activation{selu}\")\n",
    "\n",
    "selu_deriv = np.where(z>0, lambd, lambd * (alpha * np.exp(z)))\n",
    "\n",
    "print(f'\\nselu_deriv {selu_deriv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
