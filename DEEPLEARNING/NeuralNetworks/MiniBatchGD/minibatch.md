### MiniBatch Gradient Descent

`MiniBatchNN.pkl`

Trained for 1000 Epochs on FashionMNIST

>_Where epochs is defined as 1000 passes of the entire dataset, not each mini-batch_

- Final Training Accuracy: 88.67%
- Final Loss: .32268733360438945

`BatchNN.pkl`

Trained for 1000 Epochs on FashionMNIST

- Final Training Accuracy: 83.593333333333%
- Final Loss: .4674918742195496


**Seems that Mini-Batch gradient descent, at least when used alongside a good batch-size, is able to allow the model to converge faster, as it's able to take more training steps per epoch.**



