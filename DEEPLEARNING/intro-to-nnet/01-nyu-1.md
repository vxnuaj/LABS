# Intro to Neural Networks

> *A recap after a 2-month long hiatus from neural networks*

## Notes on the the History, Motivation, and evolution of Deep Learning

> Learning Resource: *[Lecture: History, motivation, and evolution of Deep Learning](https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=3&t=4939s)* by NYU, Yann LeCun


### Inspiration for Deep Learning

Just as planes function similar to birds, but their details are extremely different, neural networks are inspired by the brain in the sense that they provide the same function under extremely different details.

McCollough & Pitts ($1943$) came up with the idea of binary neurons (perceptrons), where a network of binary neurons can perform logic. Essentially a boolean circuit.

<div align = 'center' width = 500>
<img src = 'https://www.researchgate.net/publication/356858632/figure/fig1/AS:1098794657693698@1638984469361/McCulloch-and-Pitts-Neuron-Model-13.ppm'/>
</div>
<br>

> *Reminds me, wouldn't this be akin to a turing machine? Given an infinite number of boolean operations, a network of binary neurons should be able to represent any function that a turing machine can.* 

Then came Donald Hebb,  who came up with hebbian synaptic plasticity (neuroplasticity or hebbian learning), stating that neuronal connections can be remodeled by experience. *"Neurons that (often) fire together, wire together"*.

Rosenblatt, then built the first hardware implementation of the perceptron, [*the Mark I Perceptron Machine*](https://en.wikipedia.org/wiki/Perceptron). 

 Potentiometers were used as the 'weights' of the perceptron. Electric motors were used to update the weights.

<div align ='center'>
<img src = 'https://upload.wikimedia.org/wikipedia/en/thumb/5/52/Mark_I_perceptron.jpeg/220px-Mark_I_perceptron.jpeg'/><br>
<em style = 'font-size:12'>The mark I perceptron</em>
</div>
<br>

Then, came winter. People noticed the perceptron was useless for the complex visual tasks people had hoped it would excel at.

But then came back propagation. 

People realized binary perceptrons weren't useful for learning, as they were essentially a step function with a gaping discontinuity.

- For backprop to work, you need a continuous & smooth function. 

- The perceptron essentially had a $0$ gradient, making it's derivative useless for a proper learning algorithm (and @ the discontinuity, gradient $= DNE$)

Ideally, you'd have a neuron that represents a continuous and smooth function. But with the computational power at the time, having a neuron that would require a weighted sum via a linear combination was too expensive and slow. 

Each activation would require $n^m$ multiplications, where $n$ is the number of neurons at the given layer and $m$ are the inputs. Prior all you had to do was **add** the given weight of a neuron if it was active (given the perceptron's step function).

Then come the modern day increase in computational power (moore's law), we're now able to easily deploy neural networks on modern day computers

**2009 - 2012:** Speech Recognition via Neural Networks blows up.

**2012**: Convolutional Neural Networks, for Image Classification

**2015/2016**: NLP

### History of Pattern Recognition & Intro to Grad Descent.

**Traditional Machine Learning**:

Image $\rightarrow$ feature extractor $\rightarrow$ trainable classifier

The entire focus was on feature extraction, *how do I extract the correct number of features such that my classifier maximizes accuracy?*

**Deep Learning**

Image $\rightarrow$ low-level features $\rightarrow$ high-level features $\rightarrow$ high-level features $\rightarrow$ trainable classifier

You learn the entire task end-to-end. Each segment has a set of trainable parameters, with a non-linearity (activation function) that allows for the 'learning' and pattern recognition.

We need the non-linearity, otherwise having multiple layers of linear combinations would be redundant, we'd still be operating within the same $\mathbb{R}^n$ space 

$I = input$<br>
$w_i =$ weights for the $ith$ layer<br>
$W =$ all $w_i$ weights, for all layers.

$I \rightarrow w_1I \rightarrow w_2I \rightarrow w_3I = WI$

You might as well compose a single perceptron at that point, with weights $W$.

Adding a non-linearity on the other hand:

$ReLU() = max(x, 0)$

$I \rightarrow ReLU(w_1I) \rightarrow ReLU(w_2I) \rightarrow \sigma(w_3I) ≠ WI$

makes a neural network ≠ a perceptron. 

Then we're able to model more complex data (images, sound, etc) for more effective pattern recognition systems.

Such an architecture then allows for function optimization via back propagation, more specific stochastic gradient descent ($SGD$).

Given loss function $L$, compute the gradient of the loss w.r.t to the $ith$ weight and perform the update rule:

$w_i = w_i - \alpha\frac{∂L(W, X)}{∂w_i}$

for all epochs in $range(e)$, where $\alpha$ is the learning rate, to update the parameter, $w_i$ within weight matrix $W$.

### Computing Gradients via Backpropagation, Hierarchical Representation of the Cortex

Sample gradient calculation.

<div align = 'center'>

$∂Z_2 =  \frac{∂L(\hat{Y}, Y)}{∂Z_2}$

$∂W_2 = (\frac{∂L(\hat{Y}, Y)}{∂Z_2})(\frac{∂Z_2}{∂W_2})$

</div>

For a real neural network, a hidden layer can look typically as:

$h = ReLU(W_1\vec{x} + \vec{b})$

where $h$ is the output, $W_1$ are the weights for the first layer, $x$ is the input vector, and $b$ is the bias.

If $\vec{x}$ is extremely large, say in the case of a 256 by 256 RGB image, where the size of $\vec{x}$ is about $196,608$, you'd need a $W$ (shape $m, n$) with $n = 196,608$ and $m = samples$.

This becomes extremely impractical for good performance.

If you want to deal with things such as images, it's better to make a hypothesis about the structure of this large matrix $W_1$, rather than constructing the full $W_1$ that connects every single input to an individual unique neuron.

Think CNNs...

The hierarchical structure of the brain's visual cortex (V1, V2, V3, V4), performs is essentially a feed forward process. Your visual cortex receives information from your eyes and the neurons infront of the retina that perform a type of 'compression'.

> *TIL that we have a blind spot in our retina.*

The neurons in the visual cortex process data via the [feed forward ventral pathway](https://www.researchgate.net/publication/357734205/figure/fig4/AS:11431281082167173@1661997220177/Structural-characteristics-of-the-visual-cortex-A-large-number-of-feedforward-and.png), pushign electrical signals to the Inferotemporal cortex, where the classification of "*oh that's grandma!*" occurs.

> *Think of the visual cortex performing convolutions and pooling, while the inferotemporal cortex performing the classification, akin to a fully connected layer in a CNN*.

### Evolution of CNNs

Kunihiko Fukushima came up with an architecture that mimicked the recognition processes occuring in the brain, eventually led to LeCun's convnet architecture.

<div align = 'center'>
<img src = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTWcZyKciourd227EsH5mT-wi7dty72houyBg&s'><br>
<em> LeCun's first CNN, LeNet</em>
</div>

Essentially has a set of shared weights in a kernel, that performs convolutions over the entire input matrix and then pools after each layer.

<div align = 'center'>
<img src = 'https://d2l.ai/_images/lenet.svg'>
</div>

Can be applied to distinguish over any type of image, for object segmentation, computer vision, robotics, self-driving cars, etc.

In 2012, ConvNets took over via AlexNet!

Over time, the depth and scale of the models became to increase (hence **deep** learning). 

- AlexNet, 8 layers $\rightarrow$ VGG, 19 layers $\rightarrow$ ResNet-50, 50 layers.

### Deep Learning & Feature Extraction

Images are compositional, meaning they're composed of different parts:

Image $\rightarrow$ objects $\rightarrow$ parts $\rightarrow$ edges / corners $\rightarrow$ pixel values

There's a clear hierarchy. At each layer, a neural network extracts the low level features, that eventually begin to form the higher level features at later layers.

Deep Learning is essentially about learning the *manifold* of a given sample such that the transformation at the final layer has extracted enough features from the *manifold* that it's able to classify the sample into the right class.

And it works extremely well.

- You can approximate any function with two layers (univ. approx.)
- Objective function is non-convex (*why does it work so well without local minima?*)
- Networks are over-parameterized, yet can still generalize. 

### Learning Representations

**Why are deep architectures more efficient?**

In the case of the n-bit parity, Hypothetically, any function can be approximated given a 2 layer network. but to approximate any function, the number of neurons we need is exponentially to the size of the input, $n$.

You need to compute more intermediary values.

If you allow yourself to use multiple layers, complexity minimizes to linear complexity, you can approximate the same function with a decreased number of neurons.

You can approximate the same function by computing less intermediary values.

#### Manifold Hypothesis

Given a dataset in the $\mathbb{R}^n$ ambient space, we can reduce the dataset to a non-linear manifold that lies on the $\mathbb{R}^{\hat{n}}$ space of a given manifold, where $\hat{n} < n$.

It's hypothesized that real world data, despite existing in a high $n$-dimensional ambient space, $\mathbb{R}^n$, actually lies on a manifold, a subspace of the $\mathbb{R}^{n}$ space, $\mathbb{R}^{\hat{n}}$.

> *Manifold learning algorithms, such as t-SNE, are powered by this manifold hypothesis*

<div align = center>
<img width = 400 src = 'https://www.researchgate.net/publication/341724327/figure/fig1/AS:896372609912832@1590723292607/Transformation-from-3-D-Samples-to-2-D-subspaces-Zhou-2016-Among-all-the-dimension.jpg'>
</div>
<br>

Then, the true dimensionality of the manifold, $\hat{n}$ can be determiend by the number of indepent features / columns. 

Given a datamatrix $X$, the upper bound of $\hat{n}$ can be determined by $rank(X)$.

Only in linear cases, where features have relationships that can be defined via linear combinations, then $rank(X) = \hat{n}$. $PCA$ can easily find the the lower dimensional, linear manifold.

But Non-linear dependencies can allow $\hat{n} < rank(X)$. This can be approximated via other methods like $t-SNE$

The ideal feature extractor that reduces $n \rightarrow \hat{n}$ is able to extract all the independent features such that each feature represents a change in a different direction of the $\mathbb{R}^{\hat{n}}$ space.