# The Geometry of Linear Equations

### Dot Product

A dot product is a multiplication between two vectors, where each element the of the first vector is multiplied correspondingly to the elements in the second vector and then summed.

The dot product then makes up a bigger Matrix Multiplication.

### Mat Mul

A matrix multiplication is a multiplication by 2 matrices, where each row of the first matrix corresponds to the dot product of each column in the second matrix.

**Example 1**

$\begin{bmatrix}a_{11}, a_{12}\\ a_{21}, a_{22}\end{bmatrix} \cdot \begin{bmatrix} b_{11}, b_{12}\\ b_{21}, b_{22} \end{bmatrix} = C$

$C = \begin{bmatrix} a_{11} \cdot b_{11} + a_{12} \cdot b_{21} & a_{11} \cdot b_{12} + a_{12} \cdot b_{22} \\ a_{21} \cdot b_{11} + a_{22} \cdot b_{21} & a_{21} \cdot b_{12} + a_{22} \cdot b_{22} \end{bmatrix}$

$\begin{bmatrix} z_{11}, z_{12}, z_{13} \\ z_{21}, z_{22}, z_{23}\end{bmatrix} \cdot \begin{bmatrix} b_{11}, b_{12} \\ b_{21}, b_{22} \\ b_{31}, b_{32} \end{bmatrix} = C$

**Example 2**

$C = \begin{bmatrix} z_{11} \cdot b_{11} + z_{12} \cdot b_{21} + z_{13} \cdot b_{31} & z_{11} \cdot b_{12} + z_{12} \cdot b_{22} + z_{13} \cdot b_{32} \\ z_{21} \cdot b_{11} + z_{22} \cdot b_{21} + z_{23} \cdot b_{31} & z_{21} \cdot b_{12} + z_{22} \cdot b_{22} + z_{23} \cdot b_{32} \end{bmatrix}$

<br>

### Systems of Lin Eq.

Then systems of Linear Equations are a set of equations with $n$ unknowns.

The goal of these systems of equations are to compute these $n$ unknowns by solving each equation for each unknown.

A system of linear equations is traditionally defined the row way as:

$2x - y = 0$

$-x + 2y = 3$

where it can be solved graphically as:

<img src = 'images/row.png' width = 300></img>

> *Note that if lines are parallel, the system has no solution. If the lines are the same, they have infinitely many solutions.*

The row picture is a means to visualize a system of linear equations, portraying each equation as a separate row.

$\begin{cases}a_{11}x_1 + a_{12}x_2 = b_1 \\a_{21}x_1 + a_{22}x_2 = b_2\end{cases}$

Each equation, depending on it's dimensionality, represents an $nth$-dimensional plane on a $nth$-dimensional plot.

When plotting this system of linear equations, the intersection between the equations represents the solution to it.

**While it can also be defined through matrices as:**

$\begin{bmatrix} 2, -1 \\ -1, 2 \end{bmatrix} \begin{bmatrix}x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}$

where this representation is a Matrix Multiplication.

It can also be defined as a linear combination as : 

$x\begin{bmatrix} 2 \\ -1 \end{bmatrix} + y \begin{bmatrix} -1 \\ 2 \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}$

which can be solved graphically as:

<img src = 'images/column.png' width = 600></img>

> [!NOTE] 
> *I'm not writing math based notes anymore, this takes too long, screw latex.*<br>
> *Will just be writing text-based pointers and principles as I go.*

The column picture is another way to visualize a system of linear equations, as a linear combination of a set of variables and coefficients (represented a column vectors).

$x_1 \begin{pmatrix} a_{11} \\ a_{21} \end{pmatrix} + x_2 \begin{pmatrix} a_{12} \\ a_{22} \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}$

The goal, when solving via linear combination, is to figure out which combination of $x_1$ and $x_2$, or which combination of each vector, yields the vector $B$.

> *It's helpful to think of this as as element wise multiplication and then a summation to get vector $B$*

Graphically solving this system of linear equations involves taking a column vector, plotting it, then adding the the next column vector, in a manner that enables us to get the vector coordinates $B$ as a final destination.

In essence, the solution is found via vector addition.

**Plotting Systems of Equations with 3 Unknowns (3 dimensional)**

Given a system of linear equations, with 3 unknowns, graphically, plotting each row would yield a plane in 3 dimensions.

Finding the intersection of these 3 planes at their given coordinates would then result in the solution for the system of linear equations, but doing so graphically is impractical and can be better solved arithmetically.

> Definitions are within the < div >s, notes continues after the < div >
___
**Non-Singular Matrix**

A non-singular matrix, $A$, is a square matrix where it's determinant is non-zero, thereby it is invertible as $A^{-1}$.

Thereby a non-singular matrix, in a linear system of equations $Ax = b$, has a unique solution for any existing vector $b$ that can be given by $x = A^{-1}+ b$, because $A^{-1}$ exists as $A$ is determinant, and $A^{-1} A = I$ (Identity Matrix) is true.

Thereby a system of linear equations is easier to compute, as $x$ is simply equivalent to $x = A^{-1}b$.

Then, the invertible matrix of a square matrix is found by

1. Finding the Determinant of the matrix
2. Finding the Adjugate of the matrix
3. Dividing as $\frac{adj}{det}$

A matrix $A$ is considered to be invertible when $AB = BA = I$

**Determinant**

To find the determinant of a $2, 2$ matrix you subtract the product of the top left and lower right with the product of the top right and lower left.

$A = \begin{bmatrix} a, b \\ c, d \end{bmatrix}$

$det(A) = ad - bc$

To find the determinant of a $3, 3$ matrix, you take each value in the first row and multiply it by determinant of the remaining rows, barring the column that the value in the first row was in when taking the determinant of the remaining rows.

Then the multiplication, done for each value in the first row (so $3$) is then added or subtracted, done in an alternating fashion.

This process is called a co-factor expansion or Laplace Expansion

$\det\begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} = a \cdot \det\begin{pmatrix} e & f \\ h & i \end{pmatrix} - b \cdot \det\begin{pmatrix} d & f \\ g & i \end{pmatrix} + c \cdot \det\begin{pmatrix} d & e \\ g & h \end{pmatrix}$

$\det\begin{pmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 5 & 6 & 0 \end{pmatrix} = 1 \cdot (1 \cdot 0 - 4 \cdot 6) - 2 \cdot (0 \cdot 0 - 4 \cdot 5) + 3 \cdot (0 \cdot 6 - 1 \cdot 5)$

This determinant can define multiple things, one of them being the invertibility of a matrix.

**Adjugate**

The $adj()$ of a square matrix is found by swapping elements on the diagonal and then changing the signs of the off-diagonal elements.

$A = \begin{bmatrix} 1, 2 \\ 3, 4 \end{bmatrix}$

$adj(A) = \begin{bmatrix} 4, -2 \\ -3, 1 \end{bmatrix}$

___

In a system of 3 linear equations, if the 3rd column vector is within the same plane of the latter 2 vectors, then it is linearly dependent on a linear combination of those latter 2 vectors.

Thereby, the vector is singular and it the full system of 3 linear equations doesn't have a unqiue solution $x$ for any vector $\vec{b}$.

> *Unique, meaning that there can only be 1 value of $x$ that yields $\vec{b}$*

Instead, we can only find a unique solution $x$ for the $b$ within the plane given and defined by the first and second column vectors. We can't find a unique solution $x$ for vectors that lie outside of that plane.

The bounds for any $\vec{b}$ depend on $Col(A)$.

$Col()$, also known as the column space of a matrix denotes all the possible outputs of a linear combination $Ax$, within $\mathbb{R}^n$, where $n$ is the $nth$-dimension.

Therefore, $\vec{b}$ must exist within $Col(A)$ and again, therefore must be within the dimensionality of $A$. If $A$ is an $m$ x $n$ matrix, $b$ must exist within $\mathbb{R}^m$.

**As a more complex example:**

If we have a hypothetical $9$-dimensional matrix $A$, denoting a system of linear equations, if the $9th$ column vector was linearly dependent on the $8th$ column vector, the $Col(A)$ wouldn't represent the output of the linear combination $Ax$.

Given that only a subset of $8$ column vectors are linearly independent, call this matrix $B$ of size $9, 8$, then an output $C$ would be within to $Col(B)$ within $\mathbb{R}^{9}$

# 1.X of Introduction to  Linear Algebra


The 4 fundamental subspaces within linear algebra are:

1. [[Column Space]]
2. [[Row Space]]
3. [[Null Space]]
4. [[Left Null Space]]

$Col()$, also known as the column space of a matrix denotes all the possible outputs of a linear combination $Ax$, within $\mathbb{R}^n$, where $n$ is the $nth$-dimension.

The row space, or $Row()$ denotes the possible space or span of the rows of a given matrix $A$.

If we had a matrix:

$A = \begin{pmatrix} 1, 2 \\ 4, 3 \\ 9, 2 \end{pmatrix}$

The individual rows would be $\begin{pmatrix}1, 2 \end{pmatrix}$, $\begin{pmatrix}4, 3 \end{pmatrix}$, and $\begin{pmatrix}9, 2 \end{pmatrix}$.

Let's call them $a$, $b$, and $c$ correspondingly.

Then, the $Row()$ is the possible span that can be covered by a linear combination of $a$, $b$, and $c$, provided that they are linearly independent.

When computing a linear combination amongst the row space, we then treat each row vector as a separate "column" in the [[Column Picture]] and the [[Row Picture]].

Or the row space can then be seen as the column space of the transpose of $A$, as $A^T$

The null space of a matrix, say $A$, is defined as the set of vectors $x$ that yield the zero vector when taking the linear combination of:

$A\vec{x}$

Thereby $A\vec{x} = \vec{0}$ must be true for any vector $\vec{x}$ to be the the $Nul(A)$

The left [[null space]] is the [[null space]] of the transpose of a matrix, say $A^T$

## Vectors and Matrices

The length of a vector, as $||x||$, can be found as the $L_2$ norm per default unless otherwise specified.

If a vector $\vec{v}$ and a vector $\vec{d}$ are linearly independent, then in a linear combination as $x\vec{v} + y\vec{d}$ can yield any $b$ so long as $b$ remains on the $\mathbb{R}^n$ where $\vec{v}$ and $\vec{d}$ are within the $\mathbb{R}^n$

### Vectors in Three Dimensions

If a vector $\vec{v}$ has 3 components, then the vector exists within $\mathbb{R}^3$.

Say we have 2 vectors, $\vec{v}$ and $\vec{w}$, both within $\mathbb{R}^3$.

A linear combination of those vectors then yield $\vec{b}$ as

<div align = 'center'>

$x\vec{v} + y\vec{w} = \vec{b}$
</div>

Given that we only have 2 vectors, we only have 2 multiplicative coefifcientes $x$ and $y$ within the linear combination. Yet the vectors $\vec{v}$ and $\vec{w}$ exist within the $\mathbb{R}^3$ space. 

So given that we only have 2 combinations of $x\vec{v}$ and $y\vec{w}$, the output vector $\vec{b}$ then is in $R^3$ but lies within the 2 dimensional subspace of $\mathbb{R}^3$, as $\mathbb{R}^2$ for any $x$ and $y$, given that we only leverage 2 dimensions, say $x$ and $y$ to transform $\vec{v}$ and $\vec{w}$ to get $\vec{b}$

If we want an output vector to fill the entire $\mathbb{R}^3$ for any coefficient, rather than just a subspace of $\mathbb{R}^3$, we need to introduce a 3rd vector in the linear combination.  


<details> <summary> Examples </summary> 
<br>
Assumign all are linearly independent

#### Example 1: 3 Vectors in ℝ²

- **Vectors**: $\vec{v}_1, \vec{v}_2, \vec{v}_3$ are in $\mathbb{R}^2$, each with 2 components.
- **Linear Combination**: $x_1 \vec{v}_1 + x_2 \vec{v}_2 + x_3 \vec{v}_3$.
- **Space**: Vectors $\vec{v}_1, \vec{v}_2, \vec{v}_3$ are in $\mathbb{R}^2$.
- **Subspace Spanned**: Can be up to 2-dimensional within $\mathbb{R}^2$

This linear combination is a combination of 3 vectors within a 2 dimensional space

#### Example 2: 2 Vectors in ℝ³

- **Vectors**: $\vec{w}_1, \vec{w}_2$ are in $\mathbb{R}^3$, each with 3 components.
- **Linear Combination**: $y_1 \vec{w}_1 + y_2 \vec{w}_2$.
- **Space**: Vectors $\vec{w}_1, \vec{w}_2$ are in $\mathbb{R}^3$.
- **Subspace Spanned**: Can be up to 2-dimensional within $\mathbb{R}^3$

This linear combination is a combination of 3 vectors within a 3 dimensional space

Ultimately, the subspace space that a vector exists within is determined by the number of vectors within a linear combination ( or the basis vectors ), but then the overarching space that the vector is within is determined by the the number of values it has within.

It isn't possible for a given sytstem of linear equatiosn to have more unknowns than equations. There must be a coefficient $n$ for each equation $n$, if each vector is linearly independent. Otherwise, if there are more unknowns than equations, then one of the vectors is linearly dependent on another.

</details>

### Definitions

**ROW ECHELON FORM**

For a matrix to be in Row Echelon Form:

1. If a given row isn't all $0$s, then the first non-zero number must be a $1$
2. All zero rows are at the bottom of the matrix
3. In set of two successive rows that aren't of all zeros, the first non-zero number $1$ in the second row then occurs further right of the leading $1$ in the previous row.

**REDUCED ROW ECHELON FORM**

For a matrix to be in reduced echelon form, it must have the properties defined by the Row Echelon Form with the addition that any column which contains it's first non-zero number to be a leading $1$ has a a set of $0$s in it's other entries.

If there is no space for a $0$ below the $1$ it is not in Row Echelon Form.

$\begin{pmatrix}1 & 2 & 0 & 5 \\ 0 & 0 & 1 & 3 \\ 0 & 0 & 0 & 0 \end{pmatrix}$

Any matrix in Reduced Row Echelon Form is in Row Echelon Form

Through Reduced Row Echelon Form, we're able to find the basis vectors of the row space of a matrix.

**RANK**

The $Rank()$ is a matrix is the number of linearly independent rows or linearly independent columns it has.

The row rank and the column rank of a matrix are always equal, and a $Rank()$ is never $0$ if it isn't a zero matrix.

To find the $Rank()$ of a matrix, $A$, you can put it in Row Echelon Form through Gaussian Eliminationm, which tells us the number of linearly independent rows $A$ through it's number of non-zero rows it has.

**Gaussian Elimination**

Given a matrix $A$, of:

$A = \begin{pmatrix} 2 & 1 & -1 \\ -3 & -1 & 2 \\ -2 & 1 & 2 \end{pmatrix}$

Choose the leftmost column that doesn't consist of all $0$s.

The topmost row must not have a $0$ as it's first entry. Swap any row to ensure this if needed.

Then, multiply each element of the first row by a scalar defined by $\frac{1}{a}$, where $a$ is the first element in the first row.

$\begin{pmatrix} 1 & \frac{1}{2} & -\frac{1}{2} \\ -3 & -1 & 2 \\ -2 & 1 & 2 \end{pmatrix}$

Then, add suitable multiples of the first row to the rows below so that all entries below the leading $1$ become $0$.

$\begin{pmatrix} 1 & \frac{1}{2} & -\frac{1}{2} \\ 0 & \frac{1}{2} & \frac{1}{2} \\ 0 & 2 & 1 \end{pmatrix}$

This is repeated for the sub matrices of $A$, until $A$ is in row echelon form.

$c\begin{bmatrix}1\\2\\3\end{bmatrix} + v\begin{bmatrix}-3\\1\\-2\end{bmatrix} = \begin{bmatrix}2 \\ 3 \\ -1 \end{bmatrix}$


