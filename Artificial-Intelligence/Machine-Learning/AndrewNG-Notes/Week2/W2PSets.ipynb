{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Basics with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Build a function that returns the sigmoid of a real number x.\n",
    "\n",
    "##### Use math.exp(x) for the exponential function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 4\n",
      "Output: 0.9820137900379085\n",
      "\n",
      "Input: 3\n",
      "Output: 0.9525741268224334\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "z = np.random.randint(1,5)\n",
    "a = sigmoid(z)\n",
    "print(f\"Input: {z}\")\n",
    "print(f\"Output: {a}\")\n",
    "\n",
    "z = 3 \n",
    "print(f\"\\nInput: {z}\")\n",
    "a = sigmoid(z)\n",
    "print(f\"Output: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rarely use the math librarires in deep learnign operations. In the example of the sigmoid function, if done so on vectors or matrices, they'd produce a TypeError as math.exp() isn't applicable to vectors or matrices, just scalars.\n",
    "\n",
    "##### Implement the sigmoid function using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 2\n",
      "Output: 0.8807970779778823\n",
      "\n",
      "Input: 3\n",
      "Output: 0.9525741268224334\n",
      "\n",
      "Input: \n",
      "[[2 7]\n",
      " [2 8]]\n",
      "Output: \n",
      "[[0.88079708 0.99908895]\n",
      " [0.88079708 0.99966465]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.random.randint(1,5)\n",
    "a = sigmoid(z)\n",
    "print(f\"Input: {z}\")\n",
    "print(f\"Output: {a}\")\n",
    "\n",
    "z = 3 \n",
    "print(f\"\\nInput: {z}\")\n",
    "a = sigmoid(z)\n",
    "print(f\"Output: {a}\")\n",
    "\n",
    "z = np.random.randint(1, 9, (2,2))\n",
    "print(f\"\\nInput: \\n{z}\")\n",
    "a = sigmoid(z)\n",
    "print(f\"Output: \\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. \n",
    "\n",
    "The formula is: $sigmoid_{derivative}(x)=σ′(x)=σ(x)(1−σ(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 3\n",
      "Output: 0.045176659730912\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1 - s)\n",
    "    return ds\n",
    "\n",
    "x = 3\n",
    "ds = sigmoid_grad(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[1 2 3]\n",
      "\n",
      "Output: \n",
      "[0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array ([1,2,3])\n",
    "print(f\"Input: \\n{x}\")\n",
    "ds = sigmoid_grad(x)\n",
    "print(f\"\\nOutput: \\n{ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement image2vector() that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1). \n",
    "\n",
    "For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " [[[4 7 1]\n",
      "  [8 2 7]\n",
      "  [4 2 1]]\n",
      "\n",
      " [[7 3 8]\n",
      "  [5 4 1]\n",
      "  [1 6 1]]\n",
      "\n",
      " [[4 1 6]\n",
      "  [1 7 8]\n",
      "  [5 4 8]]]\n",
      "\n",
      " Input Shape: \n",
      " (3, 3, 3)\n",
      "\n",
      "Output: \n",
      " [[4]\n",
      " [7]\n",
      " [1]\n",
      " [8]\n",
      " [2]\n",
      " [7]\n",
      " [4]\n",
      " [2]\n",
      " [1]\n",
      " [7]\n",
      " [3]\n",
      " [8]\n",
      " [5]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [6]\n",
      " [1]\n",
      " [7]\n",
      " [8]\n",
      " [5]\n",
      " [4]\n",
      " [8]]\n",
      "\n",
      "Output Shape: (27, 1)\n"
     ]
    }
   ],
   "source": [
    "def image2vector(image): \n",
    "    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1) # dim1 is set to 1, as we want a column, not a 1d array\n",
    "    # say the image.shape amounts to be 3,3,3 so you'd multiply 3*3 to get the total vals in one matrix and then *3 again to get the total vals in all 3 matrices. Say output is 27. Then, dim 0 in the new vector is equal to 27.\n",
    "    return v\n",
    "\n",
    "image = np.random.randint(1, 9, (3, 3, 3)) \n",
    "print(f\"Input:\\n {image}\")\n",
    "print(f\"\\n Input Shape: \\n {image.shape}\")\n",
    "\n",
    "v = image2vector(image)\n",
    "print(f\"\\nOutput: \\n {v}\")\n",
    "print(f\"\\nOutput Shape: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $\\frac{x}{‖x‖}$\n",
    " (dividing each row vector of x by its norm).\n",
    "\n",
    "For example, if\n",
    "\n",
    "$x= \\begin{pmatrix} 0,3,4\\\\2,6,4 \\end{pmatrix}$\n",
    "\n",
    "then\n",
    "\n",
    "$‖x‖=np.linalg.norm(x,axis=1,keepdims=True)= \\begin{pmatrix} 5 \\\\ \\sqrt{56} \\end{pmatrix}$\n",
    "\n",
    "and\n",
    "\n",
    "$x_{normalized}=\\frac{x}{‖x‖}= \\begin{pmatrix} 0, \\frac{3}{5}, \\frac{4}{5} \\\\ \\frac{2}{\\sqrt{56}}, \\frac{6}{\\sqrt56}, \\frac{4}{\\sqrt56} \\end{pmatrix}$\n",
    "\n",
    "Note that you can divide matrices of different sizes and it works fine: this is called broadcasting\n",
    "\n",
    "Exercise: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: \n",
      "[[3.74165739]\n",
      " [8.77496439]] \n",
      "\n",
      "Normalized:\n",
      "[[0.26726124 0.53452248 0.80178373]\n",
      " [0.45584231 0.56980288 0.68376346]]\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows(x):\n",
    "    X_norm = np.linalg.norm(x, axis = 1, keepdims = True)\n",
    "    X_normalized = x / X_norm\n",
    "    print(f\"Norm: \\n{X_norm} \\n\")\n",
    "    return X_normalized\n",
    "\n",
    "x = np.array([[1, 2, 3,], [4,5,6]])\n",
    "x_normalized = normalizeRows(x)\n",
    "\n",
    "print(f\"Normalized:\\n{x_normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important concept to understand in numpy is \"broadcasting\". It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official broadcasting documentation.\n",
    "\n",
    "Exercise: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "$softmax(x) = softmax\\begin{bmatrix}\n",
    "  x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "  x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "  \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "  \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "  softmax\\text{(first row of x)}  \\\\\n",
    "  softmax\\text{(second row of x)} \\\\\n",
    "  ...  \\\\\n",
    "  softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    a = np.exp(z) / np.sum(np.exp(z), axis = 1, keepdims = True)\n",
    "    return a\n",
    "\n",
    "z = np.array([[9, 2, 5, 0, 0], [7, 5, 0, 0, 0]])\n",
    "a = softmax(z)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
    "\n",
    "Reminder:\n",
    "\n",
    "The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions (\n",
    ") are from the true values (\n",
    "). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "L1 loss is defined as\n",
    "\n",
    "$L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n"
     ]
    }
   ],
   "source": [
    "def l1_loss(y, yhat):\n",
    "    l1 = np.sum(np.abs(y - yhat))\n",
    "    return l1\n",
    "\n",
    "yhat = np.array(([.9, .2, .1, .4, .9]))\n",
    "y = np.array(([1, 0, 0 ,1, 1]))\n",
    "\n",
    "l1 = l1_loss(y, yhat)\n",
    "\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Implement the numpy vectorized version of the L2 loss.\n",
    "\n",
    "$L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43\n"
     ]
    }
   ],
   "source": [
    "def l2_loss(y, yhat):\n",
    "    l2 = np.sum((y - yhat) ** 2)\n",
    "    return l2\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "l2 = l2_loss(y, yhat)\n",
    "\n",
    "print(l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
